day08-爬虫8

1、多线程完善
2、登录古诗文
	回顾：
		需求：通过代码访问登录后的页面   url
		因为访问这个url，必须有cookie才行
	模拟登录：
	（1）创建会话，首先发送post，然后发送get
	（2）首先发送get，然后再发送post，然后再发送get
	（3）发送get，在发送post，再发送get，再发送get，登陆成功
	验证码处理：
	（1）下载图片，手动输入
	（2）光学识别  OCR   开源软件   识别率也还行  但是只能识别简单的
	（3）打码平台  花钱的  云打码
	极验验证码，识别难度就大了，那本书有一些例子
3、自动识别验证码
	光学识别验证码   OCR
	指令识别   tesseract 图片名 文件名
	代码识别  
		pip install pytesseract
		pip install pillow
	打码平台（云打码）
		下载demo使用即可
4、scrapy
	是什么？是一个Python的爬虫框架，底层是用python代码写的。非常强悍，非常出名。已经为你集成好了好多功能，用户只需要将你的工作放在你自己的业务逻辑中即可。比如、多进程、多线程、去重队列等不用操心。
	http://www.baidu.com/index.html?username=goudan&pwd=1234
	http://www.baidu.com/index.html?pwd=1234&username=goudan
	安装：pip install scrapy
	认识框架
		引擎（engine）、爬虫部分（spider）、调度器（scheduler）、管道（pipeline）、下载器（downloader）
	工作原理
		见图形
	使用：
	（1）创建工程
		scrapy startproject qqq
	（2）认识目录结构
		first                 新建的工程
			first            真正的工程文件
				spiders             爬虫目录
					__init__.py
					lala.py         爬虫文件（*）
				__init__.py         包的标记
				items.py            定义数据结构的地方（*）
				middlewares.py      中间件
				pipelines.py        管道文件（*）
				settings.py         配置文件（*）
			scrapy.cfg              一般不用，工程配置文件
	（3）生成爬虫文件
		cd firstbloodpro
		scrapy genspider 爬虫名字 域名

		name：爬虫的名字
		allowed_domains: 允许的域名列表
		start_urls: 起始的url列表
		parse函数：起始url响应过来之后，回调parse函数
		response：响应对象
	（4）认识response对象
		程序跑起来
			统一到spider里面执行命令运行
			cd firstblood/firstblood/spiders
			scrapy crawl 爬虫名字
	
			到配置文件中，取消遵从robots协议，自己定制ua
		response对象
			response.text: 字符串格式内容
			response.body: 字节格式内容
			response.xpath(): 通过xpath提取内容
		提取内容，scrapy已经集成了xpath和css，可以直接使用，但是和以前的略有不同
		提取的都是selector对象，你需要调用extract()提取你要的字符串
	（5）生成固定格式文件
		scrapy crawl qiubai -o qiubai.json
		scrapy crawl qiubai -o qiubai.xml
		scrapy crawl qiubai -o qiubai.csv
		生成csv文件，因为我已经设置过，所以我的没有空行，你的没有设置，所以你的两行数据之间有空行，网上搜一下

scrapy-pipline<https://doc.scrapy.org/en/latest/topics/item-pipeline.html>